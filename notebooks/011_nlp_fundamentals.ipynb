{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c5aee3b",
   "metadata": {},
   "source": [
    "# NLP fundametals\n",
    "Steps involved :\n",
    " - Convert the words to numercial sequences\n",
    " - Create the word-numerical sequence matrix, which shows the word vector dense matrix\n",
    " - Build the RNN network (LSTM)\n",
    " - Compile, calssify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9862d175",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow_hub as hub\n",
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47178b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tensorboard_callback(experiment_name, model_name):\n",
    "    \"\"\"\n",
    "    Create an tensorboard callback.\n",
    "    \"\"\"\n",
    "    return tf.keras.callbacks.TensorBoard(log_dir=os.path.join(\"model_logs\", experiment_name, model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c6da2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_checkpoint_callback(experiment_name, model_name):\n",
    "    \"\"\"\n",
    "     Create an Model Checkpoint callback\n",
    "    \"\"\"\n",
    "    return tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(\"model_checkpoints\", experiment_name, model_name+\".ckpt\"),\n",
    "                                              save_weights_only=True, monitor=\"val_acc\", \n",
    "                                              save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b8cdd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "def calculate_results_score(y_true, y_pred):\n",
    "    model_accuracy = accuracy_score(y_true, y_pred)*100\n",
    "    print(f\"[calculate_results_score] the accuracy is :: {model_accuracy}\")\n",
    "    model_precision, model_recall, model_f1_score, _ = precision_recall_fscore_support(y_true, y_pred, average=\"weighted\")\n",
    "    print(f\"[calculate_results_score] The precision is : {model_precision}\")\n",
    "    print(f\"[calculate_results_score] The recall is : {model_recall}\")\n",
    "    print(f\"[calculate_results_score] The f1 score is : {model_f1_score}\")\n",
    "    return {\n",
    "        \"accuracy\": model_accuracy,\n",
    "        \"precision\": model_precision,\n",
    "        \"recall\": model_recall,\n",
    "        \"f1_score\": model_f1_score,\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5da2d53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(   id keyword location                                               text  \\\n",
       " 0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       " 1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       " 2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       " 3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       " 4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       " \n",
       "    target  \n",
       " 0       1  \n",
       " 1       1  \n",
       " 2       1  \n",
       " 3       1  \n",
       " 4       1  ,\n",
       "    id keyword location                                               text\n",
       " 0   0     NaN      NaN                 Just happened a terrible car crash\n",
       " 1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n",
       " 2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n",
       " 3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n",
       " 4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"../datasets/nlp-getting-started/train.csv\")\n",
    "test_df = pd.read_csv(\"../datasets/nlp-getting-started/test.csv\")\n",
    "train_df.head(), test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c94fa36b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(         id                keyword              location  \\\n",
       " 4678   6649              landslide  Melbourne, Australia   \n",
       " 3002   4313           dust%20storm                   NaN   \n",
       " 4321   6135                 hijack            Houston TX   \n",
       " 1251   1807  buildings%20on%20fire                    UK   \n",
       " 917    1327                 bloody                   AUS   \n",
       " ...     ...                    ...                   ...   \n",
       " 7069  10125               upheaval      IG/SC:bjfordiani   \n",
       " 2835   4079              displaced           Oakland, CA   \n",
       " 4310   6119               hellfire                   NaN   \n",
       " 4378   6219               hijacker           California    \n",
       " 5871   8388                   ruin                   NaN   \n",
       " \n",
       "                                                    text  target  \n",
       " 4678                 @kemal_atlay caught in a landslide       1  \n",
       " 3002  || So.... I just watched the trailed for The D...       0  \n",
       " 4321  Tension In Bayelsa As Patience Jonathan Plans ...       1  \n",
       " 1251  #TweetLikeItsSeptember11th2001 Those two build...       1  \n",
       " 917   Marlon Williams &gt; Elvis Presley &gt; Marlon...       0  \n",
       " ...                                                 ...     ...  \n",
       " 7069  But...! Oh! \\n\\nHow rich the soil?!\\n\\nHow won...       0  \n",
       " 2835  Historic flooding across Asia leaves hundreds ...       1  \n",
       " 4310  Hellfire! We donÛªt even want to think about ...       0  \n",
       " 4378  Governor weighs parole for California school b...       1  \n",
       " 5871  You can only make yourself happy. Fuck those t...       0  \n",
       " \n",
       " [7613 rows x 5 columns],\n",
       "          id       keyword                      location  \\\n",
       " 1500   4999     explosion                   Orlando, FL   \n",
       " 3241  10791       wrecked     Sunny Southern California   \n",
       " 3212  10664        wounds                           NaN   \n",
       " 1259   4137       drought                    Dubai, UAE   \n",
       " 1445   4796     evacuated    Chicagoland and the world!   \n",
       " ...     ...           ...                           ...   \n",
       " 2973   9840        trauma                           NaN   \n",
       " 2835   9412     survivors                       Orlando   \n",
       " 214     696      attacked               New Delhi,India   \n",
       " 282     917  bioterrorism  Fargo, ND | North of Normal    \n",
       " 1775   6005     hazardous                    Mobile, AL   \n",
       " \n",
       "                                                    text  \n",
       " 1500  @MrBrianORL at least they didn't try and put a...  \n",
       " 3241  Cramer: Iger's 3 words that wrecked Disney's s...  \n",
       " 3212  Palestinian rams car into Israeli soldiers wou...  \n",
       " 1259  U.S. in record hurricane drought http://t.co/W...  \n",
       " 1445  Grass fire burns past Roosevelt Wash. prompts ...  \n",
       " ...                                                 ...  \n",
       " 2973  Idk how I'm supposed to have my daughter in gy...  \n",
       " 2835  #orlando Survivors of Shanghai Ghetto reunite ...  \n",
       " 214   Cop injured in gunfight as militants attack Ud...  \n",
       " 282   Studying anthrax and bioterrorism before 7 am ...  \n",
       " 1775  @moneymaker32931 tanker truck overturned carry...  \n",
       " \n",
       " [3263 rows x 4 columns])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_shuffled = train_df.sample(frac=1, random_state=273)\n",
    "test_data_shuffled = test_df.sample(frac=1, random_state=273)\n",
    "train_data_shuffled, test_data_shuffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ddad289c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4342\n",
       "1    3271\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_shuffled.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9842f2ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7613, 3263)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data_shuffled), len(test_data_shuffled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1bb146e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting data into train and validation data\n",
    "train_sentences, val_sentences, train_labels, val_labels = train_test_split(train_data_shuffled[\"text\"].to_numpy(),\n",
    "                                                                            train_data_shuffled[\"target\"].to_numpy(),\n",
    "                                                                            test_size=0.1,\n",
    "                                                                            random_state=273)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b68907f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6851, 762, 6851, 762)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_sentences), len(val_sentences), len(train_labels), len(val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a456924a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['For those impacted by the #CalWildfires here are some great recovery tips to help you in the aftermath http://t.co/wwxbGuBww5',\n",
       "        \"HEY LOOK!!!  Kash's Foundation Live for Today got blown up on People Magazine's website!!  \\n\\nTodd Blake... http://t.co/2Fenu1SYu6\",\n",
       "        'Three-alarm fire destroys two residential buildings a car in Manchester N.H. on Sunday afternoon http://t.co/rVkyj3YUVK',\n",
       "        \".@jimmyfallon I crushed squirrel bones with a mortar and pestle for my school's bio dept. not really sure why #WorstSummerJob\",\n",
       "        \"The Next Financial Crash. 'The Writing is on the Wall'. Don't Say 'You Weren't Warned' http://t.co/H7lDx29aba\",\n",
       "        'Walmart is taking steps to keep children safe in hot vehicles. Take a look at the innovative car seat here! http://t.co/z3nEvGlUFm',\n",
       "        'Demolition Means Progress: Flint Michigan and the Fate of the American Metropolis Highsmith https://t.co/ZvoBMDxHGP',\n",
       "        \"Top story: @ViralSpell: 'Couple spend wedding day feeding 4000 Syrian refugees\\x89Û_ http://t.co/a2TIIVNjDY see more http://t.co/fW2XIfJ6Ec\",\n",
       "        'I added a video to a @YouTube playlist http://t.co/612BsbVw8K siren 1 gameplay/walkthrough part 1',\n",
       "        'SCREAMING @MariahCarey @ArianaGrande http://t.co/xxZD1nmb1i'],\n",
       "       dtype=object),\n",
       " array([1, 0, 1, 0, 0, 0, 1, 0, 0, 0]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sentences[:10], train_labels[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c361c80",
   "metadata": {},
   "source": [
    "### Tokenization vs Embeddings\n",
    "Tokenisation is the process of converting, assigning a token(a, an, tensorflow) to numbers ( 0, 1,2).\n",
    "Embedding is a represntation of relationships between tokens/words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c02184f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-26 18:39:04.393207: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error\n",
      "2022-01-26 18:39:04.393263: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: rocket\n",
      "2022-01-26 18:39:04.393276: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: rocket\n",
      "2022-01-26 18:39:04.393507: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 470.86.0\n",
      "2022-01-26 18:39:04.393554: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 470.86.0\n",
      "2022-01-26 18:39:04.393564: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 470.86.0\n",
      "2022-01-26 18:39:04.394584: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "text_vectorizer = tf.keras.layers.TextVectorization(max_tokens=None,\n",
    "                                  standardize=\"lower_and_strip_punctuation\",\n",
    "                                  split=\"whitespace\",\n",
    "                                  ngrams=None,\n",
    "                                  output_mode=\"int\",\n",
    "                                  output_sequence_length=None,\n",
    "                                  # pad_to_max_tokens=True\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "768eabc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.93212669683258"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([len(i.split()) for i in train_sentences])/len(train_sentences)\n",
    "# average length of a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f8cf5597",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_vocab_length = 10000\n",
    "max_length = 20 # can be tuned based on the average number of words in a tweet\n",
    "\n",
    "\n",
    "text_vectorizer = tf.keras.layers.TextVectorization(max_tokens=max_vocab_length,\n",
    "                                                    output_mode=\"int\",\n",
    "                                                    output_sequence_length=max_length,)\n",
    "text_vectorizer.adapt(train_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4da4c811",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorizer.adapt(train_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "07c5d318",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 20), dtype=int64, numpy=\n",
       "array([[  19,    9,    3, 8824,    1,  241,   19,    9,    1,   67,    3,\n",
       "           1,    1,    0,    0,    0,    0,    0,    0,    0]])>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vectorizer([\"this is a sample sentence, hope this is converted into a vectorized format\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "77d71359",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '[UNK]',\n",
       " 'the',\n",
       " 'a',\n",
       " 'in',\n",
       " 'to',\n",
       " 'of',\n",
       " 'and',\n",
       " 'i',\n",
       " 'is',\n",
       " 'for',\n",
       " 'on',\n",
       " 'you',\n",
       " 'my',\n",
       " 'with',\n",
       " 'it',\n",
       " 'that',\n",
       " 'at',\n",
       " 'by',\n",
       " 'this',\n",
       " 'from',\n",
       " 'be',\n",
       " 'are',\n",
       " 'was',\n",
       " 'have',\n",
       " 'like',\n",
       " 'as',\n",
       " 'up',\n",
       " 'me',\n",
       " 'just',\n",
       " 'so',\n",
       " 'but',\n",
       " 'not',\n",
       " 'amp',\n",
       " 'your',\n",
       " 'im',\n",
       " 'out',\n",
       " 'its',\n",
       " 'will',\n",
       " 'no',\n",
       " 'an',\n",
       " 'after',\n",
       " 'has',\n",
       " 'fire',\n",
       " 'all',\n",
       " 'when',\n",
       " 'if',\n",
       " 'we',\n",
       " 'get',\n",
       " 'about',\n",
       " 'now',\n",
       " 'new',\n",
       " 'via',\n",
       " 'more',\n",
       " 'what',\n",
       " 'dont',\n",
       " 'or',\n",
       " 'one',\n",
       " 'been',\n",
       " 'people',\n",
       " 'they',\n",
       " 'how',\n",
       " 'over',\n",
       " 'news',\n",
       " 'he',\n",
       " 'who',\n",
       " 'us',\n",
       " 'into',\n",
       " 'do',\n",
       " 'video',\n",
       " 'were',\n",
       " 'emergency',\n",
       " 'disaster',\n",
       " '2',\n",
       " 'can',\n",
       " 'there',\n",
       " 'than',\n",
       " 'her',\n",
       " 'police',\n",
       " 'some',\n",
       " 'still',\n",
       " 'would',\n",
       " 'crash',\n",
       " 'his',\n",
       " 'body',\n",
       " 'off',\n",
       " 'burning',\n",
       " 'back',\n",
       " 'got',\n",
       " 'why',\n",
       " 'know',\n",
       " 'california',\n",
       " 'buildings',\n",
       " 'them',\n",
       " 'had',\n",
       " 'time',\n",
       " 'suicide',\n",
       " 'storm',\n",
       " 'man',\n",
       " 'cant',\n",
       " 'see',\n",
       " 'bomb',\n",
       " 'going',\n",
       " 'nuclear',\n",
       " 'world',\n",
       " 'two',\n",
       " 'rt',\n",
       " 'first',\n",
       " 'day',\n",
       " 'youtube',\n",
       " 'our',\n",
       " 'love',\n",
       " 'dead',\n",
       " '3',\n",
       " 'their',\n",
       " 'train',\n",
       " 'go',\n",
       " 'car',\n",
       " 'today',\n",
       " 'families',\n",
       " 'attack',\n",
       " 'killed',\n",
       " 'fires',\n",
       " 'being',\n",
       " 'war',\n",
       " 'full',\n",
       " 'hiroshima',\n",
       " 'accident',\n",
       " 'u',\n",
       " 'say',\n",
       " 'may',\n",
       " 'many',\n",
       " 'life',\n",
       " 'here',\n",
       " 'think',\n",
       " 'only',\n",
       " 'down',\n",
       " 'good',\n",
       " 'last',\n",
       " 'years',\n",
       " 'want',\n",
       " 'could',\n",
       " 'watch',\n",
       " 'too',\n",
       " 'make',\n",
       " 'wildfire',\n",
       " 'home',\n",
       " 'did',\n",
       " 'year',\n",
       " 'way',\n",
       " 'those',\n",
       " 'mh370',\n",
       " 'look',\n",
       " 'army',\n",
       " 'then',\n",
       " 'really',\n",
       " 'help',\n",
       " 'collapse',\n",
       " 'because',\n",
       " 'please',\n",
       " 'mass',\n",
       " 'work',\n",
       " 'best',\n",
       " 'pm',\n",
       " 'lol',\n",
       " 'death',\n",
       " 'school',\n",
       " 'am',\n",
       " '5',\n",
       " 'bombing',\n",
       " 'youre',\n",
       " 'obama',\n",
       " 'need',\n",
       " 'fatal',\n",
       " 'even',\n",
       " 'another',\n",
       " '4',\n",
       " 'wreck',\n",
       " 'take',\n",
       " 'should',\n",
       " 'right',\n",
       " 'him',\n",
       " 'black',\n",
       " 'northern',\n",
       " 'legionnaires',\n",
       " 'let',\n",
       " 'forest',\n",
       " '1',\n",
       " 'she',\n",
       " 'never',\n",
       " 'much',\n",
       " 'latest',\n",
       " 'hot',\n",
       " 'floods',\n",
       " 'every',\n",
       " 'atomic',\n",
       " '2015',\n",
       " '\\x89Û',\n",
       " 'shit',\n",
       " 'old',\n",
       " 'live',\n",
       " 'homes',\n",
       " 'god',\n",
       " 'bomber',\n",
       " 'water',\n",
       " 'under',\n",
       " 'thats',\n",
       " 'read',\n",
       " 'japan',\n",
       " 'great',\n",
       " 'flames',\n",
       " 'fear',\n",
       " 'any',\n",
       " 'where',\n",
       " 'since',\n",
       " 'getting',\n",
       " 'flood',\n",
       " 'damage',\n",
       " 'content',\n",
       " 'come',\n",
       " 'city',\n",
       " 'while',\n",
       " 'weather',\n",
       " 'said',\n",
       " 'near',\n",
       " 'found',\n",
       " 'everyone',\n",
       " 'well',\n",
       " 'top',\n",
       " 'plan',\n",
       " 'night',\n",
       " 'next',\n",
       " 'most',\n",
       " 'malaysia',\n",
       " 'flooding',\n",
       " 'ever',\n",
       " 'debris',\n",
       " 'before',\n",
       " 'truck',\n",
       " 'state',\n",
       " 's',\n",
       " 'hope',\n",
       " 'feel',\n",
       " 'without',\n",
       " 'thunderstorm',\n",
       " 'smoke',\n",
       " 'military',\n",
       " 'injured',\n",
       " 'hit',\n",
       " 'fucking',\n",
       " 'earthquake',\n",
       " 'coming',\n",
       " 'through',\n",
       " 'severe',\n",
       " 'during',\n",
       " 'confirmed',\n",
       " 'cause',\n",
       " 'ass',\n",
       " 'which',\n",
       " 'these',\n",
       " 'theres',\n",
       " 'set',\n",
       " 'looks',\n",
       " 'bad',\n",
       " 'warning',\n",
       " 'evacuation',\n",
       " 'always',\n",
       " 'wounded',\n",
       " 'weapon',\n",
       " 'w',\n",
       " 'thunder',\n",
       " 'stop',\n",
       " 'oil',\n",
       " 'movie',\n",
       " 'missing',\n",
       " 'little',\n",
       " 'liked',\n",
       " 'weapons',\n",
       " 'sinking',\n",
       " 'says',\n",
       " 'refugees',\n",
       " 'reddit',\n",
       " 'natural',\n",
       " 'loud',\n",
       " 'lightning',\n",
       " 'injuries',\n",
       " 'check',\n",
       " 'house',\n",
       " 'heat',\n",
       " 'gonna',\n",
       " 'food',\n",
       " 'cross',\n",
       " 'boy',\n",
       " 'bloody',\n",
       " 'wreckage',\n",
       " 'wild',\n",
       " 'summer',\n",
       " 'services',\n",
       " 'murder',\n",
       " 'also',\n",
       " 'air',\n",
       " '70',\n",
       " 'until',\n",
       " 'sinkhole',\n",
       " 'service',\n",
       " 'released',\n",
       " 'rain',\n",
       " 'outbreak',\n",
       " 'made',\n",
       " 'hurricane',\n",
       " 'hes',\n",
       " 'hail',\n",
       " 'fall',\n",
       " 'face',\n",
       " 'explode',\n",
       " 'derailment',\n",
       " 'charged',\n",
       " 'bags',\n",
       " 'attacked',\n",
       " 'again',\n",
       " 'wrecked',\n",
       " 'whole',\n",
       " 'trapped',\n",
       " 'times',\n",
       " 'report',\n",
       " 'panic',\n",
       " 'island',\n",
       " 'girl',\n",
       " 'free',\n",
       " 'fatalities',\n",
       " 'family',\n",
       " 'evacuate',\n",
       " 'destroy',\n",
       " 'deaths',\n",
       " 'collided',\n",
       " 'change',\n",
       " 'bag',\n",
       " 'wind',\n",
       " 'twister',\n",
       " 'spill',\n",
       " 'screaming',\n",
       " 'saudi',\n",
       " 'ruin',\n",
       " 'rescue',\n",
       " 'photo',\n",
       " 'massacre',\n",
       " 'ive',\n",
       " 'head',\n",
       " 'does',\n",
       " 'call',\n",
       " 'burned',\n",
       " 'bridge',\n",
       " 'blood',\n",
       " 'august',\n",
       " 'around',\n",
       " 'area',\n",
       " 'ambulance',\n",
       " 'terrorist',\n",
       " 'survived',\n",
       " 'survive',\n",
       " 'stock',\n",
       " 'someone',\n",
       " 'show',\n",
       " 'rescued',\n",
       " 'post',\n",
       " 'other',\n",
       " 'mudslide',\n",
       " 'migrants',\n",
       " 'landslide',\n",
       " 'high',\n",
       " 'harm',\n",
       " 'fuck',\n",
       " 'drought',\n",
       " 'deluge',\n",
       " 'curfew',\n",
       " 'breaking',\n",
       " 'woman',\n",
       " 'white',\n",
       " 'whirlwind',\n",
       " 'trauma',\n",
       " 'things',\n",
       " 'thing',\n",
       " 'terrorism',\n",
       " 'oh',\n",
       " 'lives',\n",
       " 'keep',\n",
       " 'injury',\n",
       " 'explosion',\n",
       " 'destroyed',\n",
       " 'county',\n",
       " 'catastrophe',\n",
       " 'away',\n",
       " 'anniversary',\n",
       " 'airplane',\n",
       " '40',\n",
       " '\\x89ÛÒ',\n",
       " 'windstorm',\n",
       " 'wanna',\n",
       " 'update',\n",
       " 'traumatised',\n",
       " 'survivors',\n",
       " 'story',\n",
       " 'save',\n",
       " 'run',\n",
       " 'road',\n",
       " 'rioting',\n",
       " 'rescuers',\n",
       " 'real',\n",
       " 'quarantine',\n",
       " 'investigators',\n",
       " 'ill',\n",
       " 'hazardous',\n",
       " 'hazard',\n",
       " 'group',\n",
       " 'failure',\n",
       " 'end',\n",
       " 'danger',\n",
       " 'boat',\n",
       " 'blew',\n",
       " 'against',\n",
       " 'tonight',\n",
       " 'suspect',\n",
       " 'structural',\n",
       " 'sandstorm',\n",
       " 'reunion',\n",
       " 'quarantined',\n",
       " 'power',\n",
       " 'phone',\n",
       " 'meltdown',\n",
       " 'horrible',\n",
       " 'drowning',\n",
       " 'displaced',\n",
       " 'devastation',\n",
       " 'blown',\n",
       " 'better',\n",
       " 'twitter',\n",
       " 'trouble',\n",
       " 'thank',\n",
       " 'soon',\n",
       " 'red',\n",
       " 'possible',\n",
       " 'past',\n",
       " 'market',\n",
       " 'least',\n",
       " 'inundated',\n",
       " 'hostages',\n",
       " 'flattened',\n",
       " 'famine',\n",
       " 'evacuated',\n",
       " 'engulfed',\n",
       " 'dust',\n",
       " 'destruction',\n",
       " 'derailed',\n",
       " 'collide',\n",
       " 'collapsed',\n",
       " 'cliff',\n",
       " 'came',\n",
       " 'bus',\n",
       " 'bombed',\n",
       " 'bleeding',\n",
       " 'big',\n",
       " 'battle',\n",
       " 'bang',\n",
       " 'armageddon',\n",
       " 'wounds',\n",
       " 'went',\n",
       " 'wave',\n",
       " 'typhoon',\n",
       " 'tragedy',\n",
       " 'sunk',\n",
       " 'screamed',\n",
       " 'ok',\n",
       " 'obliterated',\n",
       " 'national',\n",
       " 'mosque',\n",
       " 'long',\n",
       " 'left',\n",
       " 'kills',\n",
       " 'isis',\n",
       " 'id',\n",
       " 'hundreds',\n",
       " 'hijacker',\n",
       " 'goes',\n",
       " 'game',\n",
       " 'exploded',\n",
       " 'desolation',\n",
       " 'derail',\n",
       " 'crush',\n",
       " 'crashed',\n",
       " 'catastrophic',\n",
       " 'baby',\n",
       " '6',\n",
       " 'women',\n",
       " 'ur',\n",
       " 'traffic',\n",
       " 'something',\n",
       " 'send',\n",
       " 'screams',\n",
       " 'saw',\n",
       " 'riot',\n",
       " 'razed',\n",
       " 'put',\n",
       " 'panicking',\n",
       " 'pandemonium',\n",
       " 'officials',\n",
       " 'obliteration',\n",
       " 'lets',\n",
       " 'lava',\n",
       " 'land',\n",
       " 'kill',\n",
       " 'hostage',\n",
       " 'heart',\n",
       " 'heard',\n",
       " 'half',\n",
       " 'government',\n",
       " 'fatality',\n",
       " 'electrocuted',\n",
       " 'drowned',\n",
       " 'drown',\n",
       " 'doing',\n",
       " 'detonate',\n",
       " 'chemical',\n",
       " 'casualties',\n",
       " 'care',\n",
       " 'building',\n",
       " 'blast',\n",
       " 'affected',\n",
       " '15',\n",
       " 'yet',\n",
       " 'week',\n",
       " 'volcano',\n",
       " 'violent',\n",
       " 'very',\n",
       " 'used',\n",
       " 'tornado',\n",
       " 'thought',\n",
       " 'song',\n",
       " 'security',\n",
       " 'prebreak',\n",
       " 'pkk',\n",
       " 'part',\n",
       " 'nothing',\n",
       " 'must',\n",
       " 'issues',\n",
       " 'iran',\n",
       " 'fun',\n",
       " 'due',\n",
       " 'done',\n",
       " 'demolition',\n",
       " 'demolish',\n",
       " 'crushed',\n",
       " 'collision',\n",
       " 'caused',\n",
       " 'bioterror',\n",
       " 'believe',\n",
       " 'beautiful',\n",
       " 'bagging',\n",
       " '9',\n",
       " 'zone',\n",
       " 'whats',\n",
       " 'use',\n",
       " 'tomorrow',\n",
       " 'south',\n",
       " 'same',\n",
       " 'remember',\n",
       " 'plane',\n",
       " 'person',\n",
       " 'north',\n",
       " 'making',\n",
       " 'kids',\n",
       " 'inside',\n",
       " 'fedex',\n",
       " 'fan',\n",
       " 'eyewitness',\n",
       " 'didnt',\n",
       " 'detonated',\n",
       " 'declares',\n",
       " 'calgary',\n",
       " 'blazing',\n",
       " 'ablaze',\n",
       " 'words',\n",
       " 'upheaval',\n",
       " 'turkey',\n",
       " 'tsunami',\n",
       " 'thanks',\n",
       " 'sure',\n",
       " 'sue',\n",
       " 'start',\n",
       " 'shoulder',\n",
       " 'seismic',\n",
       " 'river',\n",
       " 'responders',\n",
       " 'rainstorm',\n",
       " 'policy',\n",
       " 'plans',\n",
       " 'music',\n",
       " 'minute',\n",
       " 'men',\n",
       " 'media',\n",
       " 'longer',\n",
       " 'light',\n",
       " 'israeli',\n",
       " 'india',\n",
       " 'hours',\n",
       " 'hijacking',\n",
       " 'hijack',\n",
       " 'ebay',\n",
       " 'detonation',\n",
       " 'casualty',\n",
       " 'apocalypse',\n",
       " 'already',\n",
       " 'airport',\n",
       " 'wake',\n",
       " 'typhoondevastated',\n",
       " 'three',\n",
       " 'such',\n",
       " 'stay',\n",
       " 'st',\n",
       " 'sound',\n",
       " 'saipan',\n",
       " 'rubble',\n",
       " 're\\x89Û',\n",
       " 'obliterate',\n",
       " 'murderer',\n",
       " 'maybe',\n",
       " 'lot',\n",
       " 'leave',\n",
       " 'islam',\n",
       " 'hell',\n",
       " 'helicopter',\n",
       " 'guys',\n",
       " 'find',\n",
       " 'fight',\n",
       " 'few',\n",
       " 'doesnt',\n",
       " 'demolished',\n",
       " 'deluged',\n",
       " 'cyclone',\n",
       " 'cool',\n",
       " 'avalanche',\n",
       " 'arson',\n",
       " 'annihilated',\n",
       " '7',\n",
       " '16yr',\n",
       " 'yes',\n",
       " 'yeah',\n",
       " 'wont',\n",
       " 'watching',\n",
       " 'stretcher',\n",
       " 'street',\n",
       " 'site',\n",
       " 'sirens',\n",
       " 'shot',\n",
       " 'shooting',\n",
       " 'play',\n",
       " 'own',\n",
       " 'officer',\n",
       " 'nearby',\n",
       " 'memories',\n",
       " 'horror',\n",
       " 'hellfire',\n",
       " 'health',\n",
       " 'having',\n",
       " 'died',\n",
       " 'conclusively',\n",
       " 'bush',\n",
       " 'both',\n",
       " 'aircraft',\n",
       " 'actually',\n",
       " 'abc',\n",
       " 'yourself',\n",
       " 'tell',\n",
       " 'team',\n",
       " 'swallowed',\n",
       " 'support',\n",
       " 'signs',\n",
       " 'rise',\n",
       " 'place',\n",
       " 'pic',\n",
       " 'online',\n",
       " 'manslaughter',\n",
       " 'line',\n",
       " 'job',\n",
       " 'days',\n",
       " 'business',\n",
       " 'brown',\n",
       " 'bar',\n",
       " 'anything',\n",
       " 'ago',\n",
       " '20',\n",
       " '\\x89ÛÓ',\n",
       " 'waves',\n",
       " 'wait',\n",
       " 'trains',\n",
       " 'town',\n",
       " 'soudelor',\n",
       " 'snowstorm',\n",
       " 'saved',\n",
       " 'reactor',\n",
       " 'projected',\n",
       " 'pick',\n",
       " 'outside',\n",
       " 'nowplaying',\n",
       " 'mp',\n",
       " 'miners',\n",
       " 'makes',\n",
       " 'major',\n",
       " 'lab',\n",
       " 'la',\n",
       " 'hollywood',\n",
       " 'history',\n",
       " 'hear',\n",
       " 'happy',\n",
       " 'finally',\n",
       " 'far',\n",
       " 'eyes',\n",
       " 'effect',\n",
       " 'data',\n",
       " 'd',\n",
       " 'crews',\n",
       " 'country',\n",
       " 'children',\n",
       " 'case',\n",
       " 'book',\n",
       " 'bigger',\n",
       " 'amid',\n",
       " 'west',\n",
       " 'trying',\n",
       " 'transport',\n",
       " 'theyre',\n",
       " 'siren',\n",
       " 'peace',\n",
       " 'order',\n",
       " 'once',\n",
       " 'n',\n",
       " 'lost',\n",
       " 'literally',\n",
       " 'gets',\n",
       " 'feared',\n",
       " 'fast',\n",
       " 'everything',\n",
       " 'electrocute',\n",
       " 'die',\n",
       " 'desolate',\n",
       " 'crisis',\n",
       " 'child',\n",
       " 'center',\n",
       " 'bc',\n",
       " 'anyone',\n",
       " 'almost',\n",
       " '8',\n",
       " '50',\n",
       " '30',\n",
       " '11yearold',\n",
       " '11',\n",
       " 'worst',\n",
       " 'trench',\n",
       " 'though',\n",
       " 'texas',\n",
       " 'second',\n",
       " 'searching',\n",
       " 'refugio',\n",
       " 'reason',\n",
       " 'probably',\n",
       " 'pretty',\n",
       " 'poor',\n",
       " 'photos',\n",
       " 'pakistan',\n",
       " 'omg',\n",
       " 'name',\n",
       " 'myself',\n",
       " 'money',\n",
       " 'mom',\n",
       " 'lord',\n",
       " 'leather',\n",
       " 'isnt',\n",
       " 'houses',\n",
       " 'hat',\n",
       " 'feeling',\n",
       " 'east',\n",
       " 'deal',\n",
       " 'damn',\n",
       " 'costlier',\n",
       " 'control',\n",
       " 'bodies',\n",
       " 'blight',\n",
       " 'bioterrorism',\n",
       " 'bestnaijamade',\n",
       " 'annihilation',\n",
       " 'across',\n",
       " '60',\n",
       " '12',\n",
       " '10',\n",
       " 'youth',\n",
       " 'yours',\n",
       " 'womens',\n",
       " 'view',\n",
       " 'vehicle',\n",
       " 'tv',\n",
       " 'totally',\n",
       " 'toddler',\n",
       " 'taken',\n",
       " 'space',\n",
       " 'side',\n",
       " 'ship',\n",
       " 'shes',\n",
       " 'seen',\n",
       " 'reuters',\n",
       " 'reports',\n",
       " 'rd',\n",
       " 'pay',\n",
       " 'others',\n",
       " 'okay',\n",
       " 'official',\n",
       " 'offensive',\n",
       " 'morning',\n",
       " 'might',\n",
       " 'm',\n",
       " 'low',\n",
       " 'hey',\n",
       " 'hard',\n",
       " 'hailstorm',\n",
       " 'giant',\n",
       " 'gbbo',\n",
       " 'fukushima',\n",
       " 'friends',\n",
       " 'flash',\n",
       " 'flag',\n",
       " 'declaration',\n",
       " 'couple',\n",
       " 'closed',\n",
       " 'climate',\n",
       " 'caught',\n",
       " 'cars',\n",
       " 'cake',\n",
       " 'banned',\n",
       " 'angry',\n",
       " 'american',\n",
       " 'america',\n",
       " 'aftershock',\n",
       " 'wrong',\n",
       " 'village',\n",
       " 'usa',\n",
       " 'till',\n",
       " 'talk',\n",
       " 't',\n",
       " 'stand',\n",
       " 'russian',\n",
       " 'running',\n",
       " 'ready',\n",
       " 'public',\n",
       " 'picking',\n",
       " 'needs',\n",
       " 'nearly',\n",
       " 'mph',\n",
       " 'move',\n",
       " 'marks',\n",
       " 'looking',\n",
       " 'learn',\n",
       " 'ladies',\n",
       " 'issued',\n",
       " 'hate',\n",
       " 'happened',\n",
       " 'guy',\n",
       " 'gun',\n",
       " 'global',\n",
       " 'flight',\n",
       " 'emmerdale',\n",
       " 'disea',\n",
       " 'devastated',\n",
       " 'course',\n",
       " 'christian',\n",
       " 'chile',\n",
       " 'chance',\n",
       " 'centre',\n",
       " 'called',\n",
       " 'blaze',\n",
       " 'become',\n",
       " 'beach',\n",
       " 'ball',\n",
       " 'b',\n",
       " 'appears',\n",
       " 'wow',\n",
       " 'united',\n",
       " 'truth',\n",
       " 'temple',\n",
       " 'takes',\n",
       " 'self',\n",
       " 'reddits',\n",
       " 'radio',\n",
       " 'property',\n",
       " 'ppl',\n",
       " 'muslims',\n",
       " 'mount',\n",
       " 'meek',\n",
       " 'mayhem',\n",
       " 'level',\n",
       " 'instead',\n",
       " 'green',\n",
       " 'gems',\n",
       " 'following',\n",
       " 'follow',\n",
       " 'film',\n",
       " 'escape',\n",
       " 'else',\n",
       " 'drive',\n",
       " 'downtown',\n",
       " 'daily',\n",
       " 'crazy',\n",
       " 'computers',\n",
       " 'cnn',\n",
       " 'bring',\n",
       " 'blue',\n",
       " 'bbc',\n",
       " 'anthrax',\n",
       " 'alone',\n",
       " 'added',\n",
       " 'action',\n",
       " '100',\n",
       " '05',\n",
       " 'wonder',\n",
       " 'win',\n",
       " 'vs',\n",
       " 'virgin',\n",
       " 'tweet',\n",
       " 'try',\n",
       " 'trust',\n",
       " 'thursday',\n",
       " 'thousands',\n",
       " 'subreddits',\n",
       " 'star',\n",
       " 'spot',\n",
       " 'sounds',\n",
       " 'sorry',\n",
       " 'scared',\n",
       " 'r',\n",
       " 'pray',\n",
       " 'potus',\n",
       " 'playing',\n",
       " 'patience',\n",
       " 'pakistani',\n",
       " 'outrage',\n",
       " 'nws',\n",
       " 'nigerian',\n",
       " 'myanmar',\n",
       " 'moment',\n",
       " 'link',\n",
       " 'likely',\n",
       " 'libya',\n",
       " 'led',\n",
       " 'large',\n",
       " 'knock',\n",
       " 'israel',\n",
       " 'insurance',\n",
       " 'ignition',\n",
       " 'huge',\n",
       " 'hiring',\n",
       " 'heavy',\n",
       " 'haha',\n",
       " 'gt',\n",
       " 'gop',\n",
       " 'front',\n",
       " 'friend',\n",
       " 'former',\n",
       " 'favorite',\n",
       " 'experts',\n",
       " 'ebola',\n",
       " 'driver',\n",
       " 'download',\n",
       " 'colorado',\n",
       " 'coaches',\n",
       " 'chinas',\n",
       " 'china',\n",
       " 'businesses',\n",
       " 'blizzard',\n",
       " 'behind',\n",
       " 'bayelsa',\n",
       " 'awesome',\n",
       " 'aug',\n",
       " 'aint',\n",
       " '70th',\n",
       " '25',\n",
       " '13',\n",
       " 'york',\n",
       " 'unconfirmed',\n",
       " 'turned',\n",
       " 'true',\n",
       " 'tried',\n",
       " 'tote',\n",
       " 'theater',\n",
       " 'taking',\n",
       " 'super',\n",
       " 'sign',\n",
       " 'seeing',\n",
       " 'sad',\n",
       " 'rules',\n",
       " 'reported',\n",
       " 'quiz',\n",
       " 'problem',\n",
       " 'pradesh',\n",
       " ...]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vectorizer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f1ef6098",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pajamas', 'painthey', 'painful', 'paine', 'paging']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_vocab = text_vectorizer.get_vocabulary()\n",
    "words_vocab[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "033eef77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.embeddings.Embedding at 0x7f2ba746cfa0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding = tf.keras.layers.Embedding(input_dim=max_vocab_length,\n",
    "                                            output_dim=128,\n",
    "                                            input_length=max_length,\n",
    "                                            )\n",
    "embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0b438f",
   "metadata": {},
   "source": [
    "## Base Model in NaiveBayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "623c50fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidf', TfidfVectorizer()), ('clf', MultinomialNB())])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Base Line, built using non DL model\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "model_0 = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer()),\n",
    "    (\"clf\", MultinomialNB())\n",
    "])\n",
    "\n",
    "model_0.fit(train_sentences, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7b928ec1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8044619422572179"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_score = model_0.score(val_sentences, val_labels)\n",
    "base_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "70e312e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 1, 0, 0, 1, 1, 1])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model_0.predict(val_sentences)\n",
    "predictions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2f42a797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[calculate_results_score] the accuracy is :: 80.4461942257218\n",
      "[calculate_results_score] The precision is : 0.815353279638672\n",
      "[calculate_results_score] The recall is : 0.8044619422572179\n",
      "[calculate_results_score] The f1 score is : 0.7975368757761941\n"
     ]
    }
   ],
   "source": [
    "model_0_results = calculate_results_score(val_labels, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff51f7b2",
   "metadata": {},
   "source": [
    "## Base Deep NN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1507d36a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"BaseDenseModel\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_layer (InputLayer)    [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_1 (TextV  (None, 20)               0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding_1 (Embedding)     (None, 20, 128)           1280000   \n",
      "                                                                 \n",
      " global_average_pooling1d (G  (None, 128)              0         \n",
      " lobalAveragePooling1D)                                          \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,280,129\n",
      "Trainable params: 1,280,129\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = tf.keras.layers.Input(shape=(1,), dtype=\"string\", name=\"input_layer\")\n",
    "# Convert the raw words into numbers\n",
    "vectorization_layer = text_vectorizer(inputs)\n",
    "# Convert the numerical data into embeddings based on thier sequence/weights\n",
    "embedding_layer = tf.keras.layers.Embedding(input_dim=max_vocab_length,\n",
    "                                            output_dim=128,\n",
    "                                            input_length=max_length,\n",
    "                                            )(vectorization_layer)\n",
    "pooling_layer = tf.keras.layers.GlobalAveragePooling1D()(embedding_layer)\n",
    "outputs=tf.keras.layers.Dense(units=1, activation=\"sigmoid\")(pooling_layer)\n",
    "model_1 = tf.keras.Model(inputs=inputs, outputs=outputs, name=\"BaseDenseModel\")\n",
    "model_1.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=[\"accuracy\"])\n",
    "tf.keras.utils.plot_model(model=model_1, show_shapes=True)\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b63022af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "208/215 [============================>.] - ETA: 0s - loss: 0.6291 - accuracy: 0.6627WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "215/215 [==============================] - 3s 7ms/step - loss: 0.6265 - accuracy: 0.6666 - val_loss: 0.5334 - val_accuracy: 0.7861\n",
      "Epoch 2/5\n",
      "208/215 [============================>.] - ETA: 0s - loss: 0.4669 - accuracy: 0.8157WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.4664 - accuracy: 0.8146 - val_loss: 0.4470 - val_accuracy: 0.8123\n",
      "Epoch 3/5\n",
      "213/215 [============================>.] - ETA: 0s - loss: 0.3683 - accuracy: 0.8512WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.3683 - accuracy: 0.8510 - val_loss: 0.4309 - val_accuracy: 0.8202\n",
      "Epoch 4/5\n",
      "208/215 [============================>.] - ETA: 0s - loss: 0.3047 - accuracy: 0.8809WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.3043 - accuracy: 0.8809 - val_loss: 0.4359 - val_accuracy: 0.8189\n",
      "Epoch 5/5\n",
      "213/215 [============================>.] - ETA: 0s - loss: 0.2570 - accuracy: 0.9030WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "215/215 [==============================] - 1s 6ms/step - loss: 0.2573 - accuracy: 0.9028 - val_loss: 0.4572 - val_accuracy: 0.8110\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f94f9870e20>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1.fit(x=train_sentences,\n",
    "            y=train_labels,\n",
    "            epochs=5,\n",
    "            validation_data=(val_sentences, val_labels),\n",
    "            callbacks=[create_model_checkpoint_callback(\"nlp_exp\", \"base_deep_model\"),\n",
    "                       create_tensorboard_callback(\"nlp_exp\", \"base_deep_model\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ee140bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1_pred_probs = model_1.predict(val_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8d836344",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((762, 1), array([0.0884866], dtype=float32))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1_pred_probs.shape, model_1_pred_probs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2c59acb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 0., 0., 0., 1., 0., 0., 1., 1., 1.], dtype=float32)>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1_predictions = tf.squeeze(tf.round(model_1_pred_probs))\n",
    "model_1_predictions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e9a827c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[calculate_results_score] the accuracy is :: 81.10236220472441\n",
      "[calculate_results_score] The precision is : 0.8105000201894885\n",
      "[calculate_results_score] The recall is : 0.8110236220472441\n",
      "[calculate_results_score] The f1 score is : 0.8094559672226677\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 81.10236220472441,\n",
       " 'precision': 0.8105000201894885,\n",
       " 'recall': 0.8110236220472441,\n",
       " 'f1_score': 0.8094559672226677}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1_results = calculate_results_score(val_labels, model_1_predictions)\n",
    "model_1_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "55dbaf3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 80.4461942257218,\n",
       " 'precision': 0.815353279638672,\n",
       " 'recall': 0.8044619422572179,\n",
       " 'f1_score': 0.7975368757761941}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_0_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4cfe6302",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, ['', '[UNK]', 'the', 'a', 'in', 'to', 'of', 'and', 'i', 'is'])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# visualising the vocab data\n",
    "words_in_vocab = text_vectorizer.get_vocabulary()\n",
    "len(words_in_vocab), words_in_vocab[:10], "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5d530dad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 128)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_layer_weights = model_1.layers[2].get_weights()[0]\n",
    "embedding_layer_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2347757e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "def create_word_embedding_files(words_in_vocab, weights):\n",
    "    vector_file_path = os.path.join(\"model_logs\", \"nlp_exp\", \"base_deep_model\", 'vectors.tsv')\n",
    "    metadata_file_path = os.path.join(\"model_logs\", \"nlp_exp\", \"base_deep_model\", 'metadata.tsv')\n",
    "    with open(vector_file_path, \"w\", encoding=\"utf-8\") as out_v:\n",
    "        with open(metadata_file_path, \"w\", encoding=\"utf-8\") as out_m:\n",
    "            for index, word in enumerate(words_in_vocab):\n",
    "                if index == 0:\n",
    "                    continue  # skip 0, it's padding.\n",
    "                vec = weights[index]\n",
    "                out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "                out_m.write(word + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1b7fe0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_word_embedding_files(words_in_vocab, embedding_layer_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8edeb3",
   "metadata": {},
   "source": [
    "## LSTM : long short term memory (RNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0ceef6e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-26 16:04:44.651915: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8202\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "211/215 [============================>.] - ETA: 0s - loss: 0.5059 - accuracy: 0.7540WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "215/215 [==============================] - 8s 17ms/step - loss: 0.5052 - accuracy: 0.7543 - val_loss: 0.4354 - val_accuracy: 0.7940\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - ETA: 0s - loss: 0.3105 - accuracy: 0.8780WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "215/215 [==============================] - 3s 13ms/step - loss: 0.3105 - accuracy: 0.8780 - val_loss: 0.5050 - val_accuracy: 0.7782\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - ETA: 0s - loss: 0.2189 - accuracy: 0.9183WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "215/215 [==============================] - 3s 13ms/step - loss: 0.2189 - accuracy: 0.9183 - val_loss: 0.6033 - val_accuracy: 0.7638\n",
      "Epoch 4/5\n",
      "213/215 [============================>.] - ETA: 0s - loss: 0.1489 - accuracy: 0.9476WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "215/215 [==============================] - 3s 12ms/step - loss: 0.1487 - accuracy: 0.9477 - val_loss: 0.7613 - val_accuracy: 0.7612\n",
      "Epoch 5/5\n",
      "214/215 [============================>.] - ETA: 0s - loss: 0.1077 - accuracy: 0.9578WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "215/215 [==============================] - 3s 13ms/step - loss: 0.1077 - accuracy: 0.9578 - val_loss: 0.8797 - val_accuracy: 0.7428\n",
      "Model: \"lstm_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_layer (InputLayer)    [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_1 (TextV  (None, 20)               0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding_10 (Embedding)    (None, 20, 128)           1280000   \n",
      "                                                                 \n",
      " lstm_15 (LSTM)              (None, 20, 64)            49408     \n",
      "                                                                 \n",
      " lstm_16 (LSTM)              (None, 64)                33024     \n",
      "                                                                 \n",
      " lstm_model (Dense)          (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,362,497\n",
      "Trainable params: 1,362,497\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(273)\n",
    "model_name =  \"lstm_model\"\n",
    "\n",
    "inputs = tf.keras.layers.Input(shape=(1,), dtype=\"string\", name=\"input_layer\")\n",
    "# Vectorize the data, i.e convert to numerical encoding\n",
    "# Convert the raw words into numbers\n",
    "vectorization_layer_2 = text_vectorizer(inputs)\n",
    "# Convert the numerical data into embeddings based on thier sequence/weights\n",
    "embedding_layer_2 = tf.keras.layers.Embedding(input_dim=max_vocab_length,\n",
    "                                              output_dim=128,\n",
    "                                              input_length=max_length,\n",
    "                                              embeddings_initializer=\"uniform\",\n",
    "                                            )(vectorization_layer_2)\n",
    "# RNN layer \n",
    "recurrent_layer_2_a = tf.keras.layers.LSTM(units=64, return_sequences=True)(embedding_layer_2)\n",
    "recurrent_layer_2_b = tf.keras.layers.LSTM(units=64)(recurrent_layer_2_a)\n",
    "# Dense Layer\n",
    "# x = tf.keras.layers.Dense(units=64, activation=\"relu\")(recurrent_layer_2_b)\n",
    "outputs = tf.keras.layers.Dense(units=1, activation=\"sigmoid\", name=model_name)(recurrent_layer_2_b)\n",
    "model_2 = tf.keras.Model(inputs, outputs, name=model_name)\n",
    "\n",
    "model_2.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=[\"accuracy\"])\n",
    "model_2_history = model_2.fit(train_sentences, train_labels,\n",
    "                              epochs=5, validation_data=(val_sentences, val_labels),\n",
    "                              callbacks=[create_model_checkpoint_callback(\"nlp_exp\", \"base_rnn_model\"),\n",
    "                                         create_tensorboard_callback(\"nlp_exp\", \"base_rnn_model\")])\n",
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "773a602e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.04516522],\n",
       "       [0.12157019],\n",
       "       [0.05206184],\n",
       "       [0.02336265],\n",
       "       [0.99973124],\n",
       "       [0.02011085],\n",
       "       [0.01728604],\n",
       "       [0.9997726 ],\n",
       "       [0.99965596],\n",
       "       [0.99875176]], dtype=float32)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2_pred_probs = model_2.predict(val_sentences)\n",
    "model_2_pred_probs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0dc4fe36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[calculate_results_score] the accuracy is :: 74.2782152230971\n",
      "[calculate_results_score] The precision is : 0.7456836284970436\n",
      "[calculate_results_score] The recall is : 0.7427821522309711\n",
      "[calculate_results_score] The f1 score is : 0.7437210550636849\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 74.2782152230971,\n",
       " 'precision': 0.7456836284970436,\n",
       " 'recall': 0.7427821522309711,\n",
       " 'f1_score': 0.7437210550636849}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2_preds = tf.squeeze(tf.round(model_2_pred_probs))\n",
    "model_2_results = calculate_results_score(val_labels, model_2_preds)\n",
    "model_2_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2b84aa",
   "metadata": {},
   "source": [
    "### GRU cell , grated recurrent unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "10e2e02b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"gru_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_layer (InputLayer)    [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_1 (TextV  (None, 20)               0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding_21 (Embedding)    (None, 20, 128)           1280000   \n",
      "                                                                 \n",
      " gru_17 (GRU)                (None, 20, 64)            37248     \n",
      "                                                                 \n",
      " gru_18 (GRU)                (None, 64)                24960     \n",
      "                                                                 \n",
      " output_layer (Dense)        (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,342,273\n",
      "Trainable params: 1,342,273\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(273)\n",
    "# Model 3\n",
    "inputs_3 = tf.keras.layers.Input(shape=(1,), dtype=tf.string, name=\"input_layer\")\n",
    "#vectorization\n",
    "vectorization_layer_3 = text_vectorizer(inputs_3)\n",
    "# embedding\n",
    "embedding_layer_3 = tf.keras.layers.Embedding(input_dim=max_vocab_length,\n",
    "                                              output_dim=128,\n",
    "                                              input_length=max_length,\n",
    "                                              embeddings_initializer=\"uniform\",\n",
    "                                            )(vectorization_layer_3)\n",
    "gru_layer_a = tf.keras.layers.GRU(units=64, return_sequences=True)(embedding_layer_3)\n",
    "# lstm_layer = tf.keras.layers.LSTM(units=64, return_sequences=True)(gru_layer_a)\n",
    "#print(\"lstm layer :: {}\".format(lstm_layer.shape))\n",
    "gru_layer_b = tf.keras.layers.GRU(units=64)(gru_layer_a)\n",
    "# actviation_layer = tf.keras.layers.Dense(units=64, activation=\"relu\", name=\"activation_layer\")(gru_layer_b)\n",
    "# pooling_layer = tf.keras.layers.GlobalAveragePooling1D()(gru_layer_a)\n",
    "outputs_3 = tf.keras.layers.Dense(units=1, activation=\"sigmoid\", name=\"output_layer\")(gru_layer_b)\n",
    "model_3 = tf.keras.Model(inputs=inputs_3, outputs=outputs_3, name=\"gru_model\")\n",
    "model_3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0514efbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "214/215 [============================>.] - ETA: 0s - loss: 0.5342 - accuracy: 0.7253WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "215/215 [==============================] - 7s 16ms/step - loss: 0.5340 - accuracy: 0.7254 - val_loss: 0.4334 - val_accuracy: 0.8058\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - ETA: 0s - loss: 0.3219 - accuracy: 0.8711WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "215/215 [==============================] - 3s 13ms/step - loss: 0.3219 - accuracy: 0.8711 - val_loss: 0.4899 - val_accuracy: 0.7940\n",
      "Epoch 3/5\n",
      "212/215 [============================>.] - ETA: 0s - loss: 0.2247 - accuracy: 0.9189WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "215/215 [==============================] - 3s 13ms/step - loss: 0.2247 - accuracy: 0.9186 - val_loss: 0.5571 - val_accuracy: 0.7703\n",
      "Epoch 4/5\n",
      "214/215 [============================>.] - ETA: 0s - loss: 0.1626 - accuracy: 0.9442WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "215/215 [==============================] - 3s 13ms/step - loss: 0.1627 - accuracy: 0.9441 - val_loss: 0.6291 - val_accuracy: 0.7664\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - ETA: 0s - loss: 0.1297 - accuracy: 0.9558WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "215/215 [==============================] - 3s 13ms/step - loss: 0.1297 - accuracy: 0.9558 - val_loss: 0.7652 - val_accuracy: 0.7415\n"
     ]
    }
   ],
   "source": [
    "model_3.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=[\"accuracy\"])\n",
    "model_3_history = model_3.fit(train_sentences,\n",
    "                          train_labels,\n",
    "                          validation_data=(val_sentences, val_labels),\n",
    "                          epochs=5,\n",
    "                          callbacks=[create_model_checkpoint_callback(\"nlp_exp\", \"gru_model\"),\n",
    "                                     create_tensorboard_callback(\"nlp_exp\", \"gru_model\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "604b63b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.02808117],\n",
       "       [0.2096241 ],\n",
       "       [0.02792463],\n",
       "       [0.00976873],\n",
       "       [0.99871504],\n",
       "       [0.01962819],\n",
       "       [0.01452878],\n",
       "       [0.9993586 ],\n",
       "       [0.9980019 ],\n",
       "       [0.9980489 ]], dtype=float32)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_3_pred_probs = model_3.predict(val_sentences)\n",
    "model_3_pred_probs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "571f6a26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 0., 0., 0., 1., 0., 0., 1., 1., 1.], dtype=float32)>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_3_preds = tf.squeeze(tf.round(model_3_pred_probs))\n",
    "model_3_preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4653ecd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[calculate_results_score] the accuracy is :: 74.1469816272966\n",
      "[calculate_results_score] The precision is : 0.743531793576729\n",
      "[calculate_results_score] The recall is : 0.7414698162729659\n",
      "[calculate_results_score] The f1 score is : 0.7422130025392907\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 74.1469816272966,\n",
       " 'precision': 0.743531793576729,\n",
       " 'recall': 0.7414698162729659,\n",
       " 'f1_score': 0.7422130025392907}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_results_score(val_labels, model_3_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3842117",
   "metadata": {},
   "source": [
    "### Model 4 : Bidriectional Rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ec558ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"bidirectional_rnn\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_layer (InputLayer)    [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_1 (TextV  (None, 20)               0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding_2 (Embedding)     (None, 20, 128)           1280000   \n",
      "                                                                 \n",
      " bidirectional_2 (Bidirectio  (None, 20, 128)          98816     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " bidirectional_3 (Bidirectio  (None, 128)              74496     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " output_layer (Dense)        (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,453,441\n",
      "Trainable params: 1,453,441\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(273)\n",
    "inputs_4 = tf.keras.layers.Input(shape=(1, ), dtype=tf.string, name=\"input_layer\")\n",
    "vectorization_layer_4 = text_vectorizer(inputs_4)\n",
    "embedding_layer_4 = tf.keras.layers.Embedding(input_dim=max_vocab_length,\n",
    "                                            output_dim=128,\n",
    "                                            input_length=max_length,\n",
    "                                            embeddings_initializer=\"uniform\",                                            \n",
    "                                           )(vectorization_layer_4)\n",
    "embedding_layer_4_a = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True))(embedding_layer_4)\n",
    "embedding_layer_4_b = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(64))(embedding_layer_4_a)\n",
    "outputs_4 = tf.keras.layers.Dense(units=1, activation=\"sigmoid\", name=\"output_layer\")(embedding_layer_4_b)\n",
    "\n",
    "model_4 = tf.keras.Model(inputs=inputs_4, outputs=outputs_4, name=\"bidirectional_rnn\")\n",
    "model_4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9351cee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_4.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "40d9ac71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "215/215 [==============================] - ETA: 0s - loss: 0.5077 - accuracy: 0.7485WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "215/215 [==============================] - 15s 46ms/step - loss: 0.5077 - accuracy: 0.7485 - val_loss: 0.4369 - val_accuracy: 0.7913\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - ETA: 0s - loss: 0.2995 - accuracy: 0.8781WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "215/215 [==============================] - 9s 40ms/step - loss: 0.2995 - accuracy: 0.8781 - val_loss: 0.4893 - val_accuracy: 0.7795\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - ETA: 0s - loss: 0.1951 - accuracy: 0.9275WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "215/215 [==============================] - 9s 42ms/step - loss: 0.1951 - accuracy: 0.9275 - val_loss: 0.6314 - val_accuracy: 0.7664\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - ETA: 0s - loss: 0.1264 - accuracy: 0.9574WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "215/215 [==============================] - 9s 40ms/step - loss: 0.1264 - accuracy: 0.9574 - val_loss: 0.6143 - val_accuracy: 0.7717\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - ETA: 0s - loss: 0.0974 - accuracy: 0.9661WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "215/215 [==============================] - 9s 41ms/step - loss: 0.0974 - accuracy: 0.9661 - val_loss: 0.7402 - val_accuracy: 0.7730\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2b5a8b6430>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_4.fit(train_sentences, train_labels, validation_data=(val_sentences, val_labels),\n",
    "            epochs=5, callbacks=[create_model_checkpoint_callback(\"nlp_exp\", \"bidirectional_rnn\"),\n",
    "                                 create_tensorboard_callback(\"nlp_exp\", \"bidirectional_rnn\")] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ad79a01f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01345822],\n",
       "       [0.05456653],\n",
       "       [0.02790022],\n",
       "       [0.02769393],\n",
       "       [0.99898446],\n",
       "       [0.003234  ],\n",
       "       [0.00366732],\n",
       "       [0.9999149 ],\n",
       "       [0.99889207],\n",
       "       [0.96837026]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_4_pred_probs = model_4.predict(val_sentences)\n",
    "model_4_pred_probs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ec63b3bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 0., 0., 0., 1., 0., 0., 1., 1., 1.], dtype=float32)>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_4_preds = tf.squeeze(tf.round(model_4_pred_probs))\n",
    "model_4_preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "106f59d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[calculate_results_score] the accuracy is :: 77.29658792650919\n",
      "[calculate_results_score] The precision is : 0.7719745677537196\n",
      "[calculate_results_score] The recall is : 0.7729658792650919\n",
      "[calculate_results_score] The f1 score is : 0.7703610768366989\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 77.29658792650919,\n",
       " 'precision': 0.7719745677537196,\n",
       " 'recall': 0.7729658792650919,\n",
       " 'f1_score': 0.7703610768366989}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_results_score(val_labels, model_4_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb4f3db",
   "metadata": {},
   "source": [
    "### Model 5 : CNN for text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a0592638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"cnn_text_classifier\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_layer (InputLayer)    [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_1 (TextV  (None, 20)               0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding_6 (Embedding)     (None, 20, 128)           1280000   \n",
      "                                                                 \n",
      " convolution_layer (Conv1D)  (None, 16, 64)            41024     \n",
      "                                                                 \n",
      " max_pooling_layer (GlobalMa  (None, 64)               0         \n",
      " xPooling1D)                                                     \n",
      "                                                                 \n",
      " output_layer (Dense)        (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,321,089\n",
      "Trainable params: 1,321,089\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(273)\n",
    "\n",
    "inputs_5 = tf.keras.layers.Input(shape=(1,),dtype=tf.string, name=\"input_layer\")\n",
    "vectorization_layer_5 = text_vectorizer(inputs_5)\n",
    "embedding_layer_5 = tf.keras.layers.Embedding(input_dim=max_vocab_length,\n",
    "                                            output_dim=128,\n",
    "                                            input_length=max_length,\n",
    "                                            embeddings_initializer=\"uniform\",)(vectorization_layer_5)\n",
    "conv_layer = tf.keras.layers.Conv1D(filters=64,\n",
    "                                    kernel_size=5,\n",
    "                                    strides=1,\n",
    "                                    activation=\"relu\",\n",
    "                                    padding=\"valid\", name=\"convolution_layer\")(embedding_layer_5)\n",
    "pooling_layer_5 = tf.keras.layers.GlobalMaxPool1D(name=\"max_pooling_layer\")(conv_layer)\n",
    "outputs_5 = tf.keras.layers.Dense(units=1, activation=\"sigmoid\", name=\"output_layer\")(pooling_layer_5)\n",
    "\n",
    "model_5 = tf.keras.Model(inputs=inputs_5, outputs=outputs_5, name=\"cnn_text_classifier\")\n",
    "model_5.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=[\"accuracy\"])\n",
    "model_5.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "401fdd9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "213/215 [============================>.] - ETA: 0s - loss: 0.5560 - accuracy: 0.7221WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "215/215 [==============================] - 4s 18ms/step - loss: 0.5551 - accuracy: 0.7227 - val_loss: 0.4258 - val_accuracy: 0.8189\n",
      "Epoch 2/5\n",
      "213/215 [============================>.] - ETA: 0s - loss: 0.3312 - accuracy: 0.8634WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "215/215 [==============================] - 4s 17ms/step - loss: 0.3315 - accuracy: 0.8634 - val_loss: 0.4505 - val_accuracy: 0.8176\n",
      "Epoch 3/5\n",
      "212/215 [============================>.] - ETA: 0s - loss: 0.2043 - accuracy: 0.9233WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "215/215 [==============================] - 4s 18ms/step - loss: 0.2040 - accuracy: 0.9234 - val_loss: 0.5287 - val_accuracy: 0.7953\n",
      "Epoch 4/5\n",
      "212/215 [============================>.] - ETA: 0s - loss: 0.1266 - accuracy: 0.9567WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "215/215 [==============================] - 4s 17ms/step - loss: 0.1263 - accuracy: 0.9569 - val_loss: 0.6076 - val_accuracy: 0.7913\n",
      "Epoch 5/5\n",
      "214/215 [============================>.] - ETA: 0s - loss: 0.0914 - accuracy: 0.9701WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "215/215 [==============================] - 4s 18ms/step - loss: 0.0914 - accuracy: 0.9701 - val_loss: 0.6636 - val_accuracy: 0.7808\n"
     ]
    }
   ],
   "source": [
    "model_5_history = model_5.fit(train_sentences,\n",
    "                              train_labels,\n",
    "                              epochs=5,\n",
    "                              validation_data=(val_sentences, val_labels),\n",
    "                              callbacks=[create_model_checkpoint_callback(\"nlp_exp\", \"cnn_text_classfier\"),\n",
    "                                         create_tensorboard_callback(\"nlp_exp\", \"cnn_text_classfier\"),\n",
    "                                         ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "526caea5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.4668882e-02],\n",
       "       [3.6546665e-01],\n",
       "       [3.7992895e-03],\n",
       "       [1.8065184e-02],\n",
       "       [9.9976707e-01],\n",
       "       [1.5287101e-02],\n",
       "       [6.1517954e-04],\n",
       "       [1.0000000e+00],\n",
       "       [9.9959135e-01],\n",
       "       [9.7741640e-01]], dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_5_preds_probs = model_5.predict(val_sentences)\n",
    "model_5_preds_probs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2efe90ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_5_preds = tf.squeeze(tf.round(model_5_preds_probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "afc5cf69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[calculate_results_score] the accuracy is :: 78.08398950131233\n",
      "[calculate_results_score] The precision is : 0.7796719357517247\n",
      "[calculate_results_score] The recall is : 0.7808398950131233\n",
      "[calculate_results_score] The f1 score is : 0.7796474006201792\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 78.08398950131233,\n",
       " 'precision': 0.7796719357517247,\n",
       " 'recall': 0.7808398950131233,\n",
       " 'f1_score': 0.7796474006201792}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_5_results = calculate_results_score(val_labels, model_5_preds)\n",
    "model_5_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a9fe10",
   "metadata": {},
   "source": [
    "### Model 6 : Transfer Learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6064c948",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "\n",
    "# pre trained embedding layer\n",
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "58450b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_samples = embed([\"Hey, thats a storm out there, red alert!\",\n",
    "                        \"That food was suh a disaster, it sucked.\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "49b9722a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 512), dtype=float32, numpy=\n",
       "array([[ 0.07470109,  0.05868598, -0.0422262 , ..., -0.04076605,\n",
       "         0.04728632,  0.02345333],\n",
       "       [ 0.02443152, -0.02388306,  0.06637954, ..., -0.03705139,\n",
       "         0.01797296, -0.07725956]], dtype=float32)>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bba0baa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_encoder_layer = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\",\n",
    "                                        input_shape=[],\n",
    "                                        dtype=tf.string,\n",
    "                                        trainable=False,\n",
    "                                        name=\"universal_sentence_encoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "91a67049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"pretrained_use_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " universal_sentence_encoder   (None, 512)              256797824 \n",
      " (KerasLayer)                                                    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                32832     \n",
      "                                                                 \n",
      " output_layer (Dense)        (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 256,830,721\n",
      "Trainable params: 32,897\n",
      "Non-trainable params: 256,797,824\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_6 = tf.keras.Sequential(layers=[\n",
    "    sentence_encoder_layer,\n",
    "    tf.keras.layers.Dense(units=64, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(units=1, activation=\"sigmoid\", name=\"output_layer\")\n",
    "], name=\"pretrained_use_model\")\n",
    "model_6.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "32b8e247",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_6.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "410d68aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "213/215 [============================>.] - ETA: 0s - loss: 0.5136 - accuracy: 0.7757WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "215/215 [==============================] - 4s 13ms/step - loss: 0.5127 - accuracy: 0.7762 - val_loss: 0.4219 - val_accuracy: 0.8281\n",
      "Epoch 2/5\n",
      "213/215 [============================>.] - ETA: 0s - loss: 0.4171 - accuracy: 0.8156WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "215/215 [==============================] - 3s 12ms/step - loss: 0.4172 - accuracy: 0.8155 - val_loss: 0.4194 - val_accuracy: 0.8268\n",
      "Epoch 3/5\n",
      "211/215 [============================>.] - ETA: 0s - loss: 0.4016 - accuracy: 0.8209WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "215/215 [==============================] - 3s 12ms/step - loss: 0.4028 - accuracy: 0.8205 - val_loss: 0.4215 - val_accuracy: 0.8255\n",
      "Epoch 4/5\n",
      "214/215 [============================>.] - ETA: 0s - loss: 0.3932 - accuracy: 0.8264WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "215/215 [==============================] - 3s 12ms/step - loss: 0.3933 - accuracy: 0.8263 - val_loss: 0.4256 - val_accuracy: 0.8268\n",
      "Epoch 5/5\n",
      "213/215 [============================>.] - ETA: 0s - loss: 0.3872 - accuracy: 0.8314WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "215/215 [==============================] - 3s 12ms/step - loss: 0.3879 - accuracy: 0.8310 - val_loss: 0.4243 - val_accuracy: 0.8215\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2b22cfecd0>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_6.fit(train_sentences,\n",
    "            train_labels,\n",
    "            validation_data=(val_sentences, val_labels),\n",
    "           epochs=5,\n",
    "            callbacks=[create_model_checkpoint_callback(\"nlp_exp\", \"pretrained_use_model\"),\n",
    "                       create_tensorboard_callback(\"nlp_exp\", \"pretrained_use_model\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "dd015ae9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.11684072],\n",
       "       [0.14856562],\n",
       "       [0.17748833],\n",
       "       [0.05267456],\n",
       "       [0.9908987 ],\n",
       "       [0.08122388],\n",
       "       [0.20797008],\n",
       "       [0.98698425],\n",
       "       [0.9149903 ],\n",
       "       [0.920263  ]], dtype=float32)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_6_pred_prods = model_6.predict(val_sentences)\n",
    "model_6_pred_prods[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d81ab486",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 0., 0., 0., 1., 0., 0., 1., 1., 1.], dtype=float32)>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_6_preds = tf.squeeze(tf.round(model_6_pred_prods))\n",
    "model_6_preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f4f86769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[calculate_results_score] the accuracy is :: 82.1522309711286\n",
      "[calculate_results_score] The precision is : 0.8208411032035219\n",
      "[calculate_results_score] The recall is : 0.821522309711286\n",
      "[calculate_results_score] The f1 score is : 0.8208097274071411\n"
     ]
    }
   ],
   "source": [
    "model_6_results = calculate_results_score(val_labels, model_6_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b8b15c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8ea089",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
